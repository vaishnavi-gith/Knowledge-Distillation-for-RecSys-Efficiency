# Knowledge-Distillation-for-RecSys-Efficiency
**Knowledge-Distillation-for-RecSys-Efficiency:** Explored **Knowledge Distillation** for recommendation systems, achieving **49.5% model compression** (97.5K→49.5K params). Observed **Negative Knowledge Transfer** (Recall@10: 0.097→0.091), revealing challenges in distilling soft scores for sparse ranking tasks.
