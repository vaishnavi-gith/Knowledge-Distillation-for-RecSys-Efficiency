{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaishnavi-gith/Knowledge-Distillation-for-RecSys-Efficiency/blob/main/KD_Recsys_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install torch-snippets\n",
        "\n",
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lVslOHFe4d_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration Constants ---\n",
        "NUM_USERS = 1000  # Number of users\n",
        "NUM_ITEMS = 500   # Number of items\n",
        "EMBED_SIZE = 64   # Embedding dimension for Teacher\n",
        "EMBED_SIZE_STUDENT = 32 # Smaller embedding dimension for Student\n",
        "\n",
        "# --- Data Simulation Function ---\n",
        "def generate_synthetic_data(num_users, num_items, num_interactions=50000):\n",
        "    \"\"\"Generates synthetic (user, item) interaction pairs.\"\"\"\n",
        "    users = np.random.randint(0, num_users, num_interactions)\n",
        "    items = np.random.randint(0, num_items, num_interactions)\n",
        "\n",
        "    # Create a set of unique positive interactions for BPR sampling\n",
        "    interactions = list(set(zip(users, items)))\n",
        "\n",
        "    # Map users and items to unique IDs (0 to N-1)\n",
        "    user_map = {uid: i for i, uid in enumerate(np.unique(users))}\n",
        "    item_map = {iid: i for i, iid in enumerate(np.unique(items))}\n",
        "\n",
        "    mapped_interactions = [(user_map[u], item_map[i]) for u, i in interactions]\n",
        "\n",
        "    print(f\"Generated {len(mapped_interactions)} unique positive interactions.\")\n",
        "    return mapped_interactions, len(user_map), len(item_map)\n",
        "\n",
        "# --- BPR Dataset Class ---\n",
        "class BPRDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for Bayesian Personalized Ranking (BPR).\n",
        "    Yields (user, positive_item, negative_item) triplets.\n",
        "    \"\"\"\n",
        "    def __init__(self, interactions, num_items):\n",
        "        self.interactions = interactions # List of (user_id, item_id)\n",
        "        self.num_items = num_items\n",
        "        self.user_to_items = {}\n",
        "        for u, i in interactions:\n",
        "            self.user_to_items.setdefault(u, set()).add(i)\n",
        "\n",
        "        self.users = list(self.user_to_items.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        # We sample based on the number of positive interactions\n",
        "        return len(self.interactions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Select a random user from the positive interactions\n",
        "        u, i_pos = self.interactions[idx]\n",
        "\n",
        "        # Sample a negative item (i_neg) that the user has not interacted with\n",
        "        i_neg = np.random.randint(0, self.num_items)\n",
        "        while i_neg in self.user_to_items.get(u, set()):\n",
        "            i_neg = np.random.randint(0, self.num_items)\n",
        "\n",
        "        return torch.tensor(u, dtype=torch.long), \\\n",
        "               torch.tensor(i_pos, dtype=torch.long), \\\n",
        "               torch.tensor(i_neg, dtype=torch.long)\n",
        "\n",
        "# Generate data\n",
        "interactions, actual_num_users, actual_num_items = generate_synthetic_data(NUM_USERS, NUM_ITEMS)\n",
        "train_dataset = BPRDataset(interactions, actual_num_items)\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2)\n",
        "\n",
        "print(f\"Dataset Size: {len(train_dataset)} triplets.\")"
      ],
      "metadata": {
        "id": "QvXz_hB_BSrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Architectures ---\n",
        "\n",
        "class MatrixFactorization(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard Matrix Factorization (MF) model.\n",
        "    The complexity is determined by embed_size.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_users, num_items, embed_size):\n",
        "        super().__init__()\n",
        "        self.user_embeddings = nn.Embedding(num_users, embed_size)\n",
        "        self.item_embeddings = nn.Embedding(num_items, embed_size)\n",
        "        self.user_bias = nn.Embedding(num_users, 1)\n",
        "        self.item_bias = nn.Embedding(num_items, 1)\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.normal_(self.user_embeddings.weight, std=0.01)\n",
        "        nn.init.normal_(self.item_embeddings.weight, std=0.01)\n",
        "        nn.init.zeros_(self.user_bias.weight)\n",
        "        nn.init.zeros_(self.item_bias.weight)\n",
        "\n",
        "    def forward(self, user_ids, item_ids):\n",
        "        u_embed = self.user_embeddings(user_ids)\n",
        "        i_embed = self.item_embeddings(item_ids)\n",
        "\n",
        "        u_bias = self.user_bias(user_ids).squeeze()\n",
        "        i_bias = self.item_bias(item_ids).squeeze()\n",
        "\n",
        "        # Output logit score (pre-activation, required for BPR/Distillation)\n",
        "        logit_scores = (u_embed * i_embed).sum(dim=1) + u_bias + i_bias\n",
        "        return logit_scores\n",
        "\n",
        "# Instantiate models\n",
        "teacher_model = MatrixFactorization(actual_num_users, actual_num_items, EMBED_SIZE).to(device)\n",
        "student_model = MatrixFactorization(actual_num_users, actual_num_items, EMBED_SIZE_STUDENT).to(device)\n",
        "\n",
        "print(f\"Teacher Params: {sum(p.numel() for p in teacher_model.parameters()):,}\")\n",
        "print(f\"Student Params: {sum(p.numel() for p in student_model.parameters()):,}\")"
      ],
      "metadata": {
        "id": "LAlq9Gi7BYWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Distillation Loss Function ---\n",
        "\n",
        "def distillation_loss_bpr(teacher_logits_diff, student_logits_diff, T=2.0, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Combined BPR Hard Loss and KL Divergence Soft Loss for distillation.\n",
        "\n",
        "    Args:\n",
        "        teacher_logits_diff (tensor): Teacher's score_pos - score_neg.\n",
        "        student_logits_diff (tensor): Student's score_pos - score_neg.\n",
        "        T (float): Temperature for smoothing the soft targets.\n",
        "        alpha (float): Weighting parameter for the soft loss.\n",
        "\n",
        "    Returns:\n",
        "        tensor: The combined distillation loss.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Hard Loss (Standard BPR for the Student)\n",
        "    # BPR Loss = -log(sigmoid(score_pos - score_neg))\n",
        "    # This guides the student to rank the positive item higher than the negative item.\n",
        "    hard_loss = -F.logsigmoid(student_logits_diff).mean()\n",
        "\n",
        "    # 2. Soft Loss (Knowledge Distillation Term)\n",
        "\n",
        "    # In BPR, the 'knowledge' is the difference in confidence (score_pos - score_neg).\n",
        "    # We use Mean Squared Error (MSE) to force the Student's confidence difference\n",
        "    # to match the Teacher's confidence difference (often scaled by T*T).\n",
        "\n",
        "    # Detach the teacher's output to prevent updating its weights.\n",
        "    soft_loss = F.mse_loss(student_logits_diff, teacher_logits_diff.detach()) * (T * T)\n",
        "\n",
        "    # 3. Combined Loss\n",
        "    combined_loss = (1.0 - alpha) * hard_loss + alpha * soft_loss\n",
        "\n",
        "    return combined_loss"
      ],
      "metadata": {
        "id": "Ce8H5WMRBcIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Configuration ---\n",
        "EPOCHS_TEACHER = 5\n",
        "EPOCHS_STUDENT = 10\n",
        "LR_TEACHER = 1e-3\n",
        "LR_STUDENT = 1e-3\n",
        "T = 3.0       # Distillation temperature\n",
        "ALPHA = 0.7   # Weighting factor for soft loss (0.7 means 70% soft, 30% hard)\n",
        "\n",
        "# --- Training Function ---\n",
        "def train_model(model, loader, lr, epochs, optimizer_class=optim.Adam):\n",
        "    optimizer = optimizer_class(model.parameters(), lr=lr)\n",
        "    model.train()\n",
        "\n",
        "    print(f\"\\n--- Training {model.__class__.__name__} (Hard Loss Only) ---\")\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(loader, desc=f\"E {epoch+1}/{epochs}\")\n",
        "        for u, i_pos, i_neg in pbar:\n",
        "            u, i_pos, i_neg = u.to(device), i_pos.to(device), i_neg.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Standard BPR: score_pos vs score_neg\n",
        "            pos_scores = model(u, i_pos)\n",
        "            neg_scores = model(u, i_neg)\n",
        "\n",
        "            # BPR Loss = -log(sigmoid(score_pos - score_neg))\n",
        "            diff = pos_scores - neg_scores\n",
        "            hard_loss = -F.logsigmoid(diff).mean()\n",
        "\n",
        "            hard_loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += hard_loss.item()\n",
        "            pbar.set_postfix(loss=total_loss / (pbar.n + 1))\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Average Loss: {total_loss / len(loader):.4f}\")\n",
        "    return model\n",
        "\n",
        "# --- Distillation Training Function ---\n",
        "def train_distilled_student(teacher, student, loader, lr, epochs, T, alpha, optimizer_class=optim.Adam):\n",
        "    teacher.eval() # Teacher is FIXED\n",
        "    student.train()\n",
        "    optimizer = optimizer_class(student.parameters(), lr=lr)\n",
        "\n",
        "    print(f\"\\n--- Distilling Knowledge into Student (T={T}, alpha={alpha}) ---\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(loader, desc=f\"E {epoch+1}/{epochs}\")\n",
        "        for u, i_pos, i_neg in pbar:\n",
        "            u, i_pos, i_neg = u.to(device), i_pos.to(device), i_neg.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 1. Get Teacher's Knowledge (Difference in confidence)\n",
        "            with torch.no_grad():\n",
        "                t_pos = teacher(u, i_pos)\n",
        "                t_neg = teacher(u, i_neg)\n",
        "                teacher_diff = t_pos - t_neg # Teacher's BPR difference score\n",
        "\n",
        "            # 2. Get Student's Predictions\n",
        "            s_pos = student(u, i_pos)\n",
        "            s_neg = student(u, i_neg)\n",
        "            student_diff = s_pos - s_neg # Student's BPR difference score\n",
        "\n",
        "            # 3. Calculate Distillation Loss\n",
        "            loss = distillation_loss_bpr(teacher_diff, student_diff, T=T, alpha=alpha)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix(loss=total_loss / (pbar.n + 1))\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Average Loss: {total_loss / len(loader):.4f}\")\n",
        "    return student\n",
        "\n",
        "# --- EXECUTION ---\n",
        "\n",
        "# 1. Train the Teacher (High-capacity model)\n",
        "trained_teacher = train_model(teacher_model, train_loader, LR_TEACHER, EPOCHS_TEACHER)\n",
        "\n",
        "# 2. Train the Standard Student (Small model, no distillation, for baseline)\n",
        "standard_student = MatrixFactorization(actual_num_users, actual_num_items, EMBED_SIZE_STUDENT).to(device)\n",
        "trained_standard_student = train_model(standard_student, train_loader, LR_STUDENT, EPOCHS_STUDENT)\n",
        "\n",
        "# 3. Train the Distilled Student (Small model, with teacher knowledge)\n",
        "distilled_student = MatrixFactorization(actual_num_users, actual_num_items, EMBED_SIZE_STUDENT).to(device)\n",
        "trained_distilled_student = train_distilled_student(\n",
        "    trained_teacher, distilled_student, train_loader, LR_STUDENT, EPOCHS_STUDENT, T=T, alpha=ALPHA\n",
        ")\n",
        "\n",
        "print(\"\\n--- Training Complete ---\")\n",
        "print(\"You now have three models to compare empirically (Recall@K, Latency):\")\n",
        "print(\"- Teacher: High Performance, Slow\")\n",
        "print(\"- Standard Student: Low Performance, Fast\")\n",
        "print(\"- Distilled Student: High Performance, Fast (The desired outcome)\")"
      ],
      "metadata": {
        "id": "2QyRCyShBgig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Evaluation Configuration ---\n",
        "K = 10 # Standard cutoff for recommendation evaluation\n",
        "NUM_LATENCY_TESTS = 100 # Number of times to run prediction for averaging\n",
        "\n",
        "# --- Evaluation Function ---\n",
        "def evaluate_model(model, interaction_data, num_users, num_items, k):\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Prepare Test Data (Simple Leave-One-Out setup)\n",
        "    # Since we didn't explicitly split test data, we'll use a random sample\n",
        "    # of users to test full ranking (predicting all items).\n",
        "\n",
        "    # Map interactions for testing: user -> set of positive items\n",
        "    user_to_pos_items = {}\n",
        "    for u, i in interaction_data:\n",
        "        user_to_pos_items.setdefault(u, set()).add(i)\n",
        "\n",
        "    test_users = list(user_to_pos_items.keys())\n",
        "\n",
        "    total_recall = 0\n",
        "    total_ndcg = 0\n",
        "    total_users = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for user_id in tqdm(test_users, desc=\"Evaluating Recall & NDCG\"):\n",
        "            # Select a random positive item as the 'target' for evaluation\n",
        "            # (Simulating a leave-one-out setup)\n",
        "            positive_items = list(user_to_pos_items[user_id])\n",
        "            if not positive_items:\n",
        "                continue\n",
        "\n",
        "            target_item = positive_items[np.random.randint(len(positive_items))]\n",
        "\n",
        "            # Predict scores for ALL items (0 to num_items-1)\n",
        "            user_tensor = torch.tensor([user_id] * num_items, dtype=torch.long).to(device)\n",
        "            all_item_tensor = torch.arange(num_items, dtype=torch.long).to(device)\n",
        "\n",
        "            scores = model(user_tensor, all_item_tensor)\n",
        "\n",
        "            # Mask out the positive items used in training/context (optional, but good practice)\n",
        "            # Since we didn't explicitly hold out a separate test set, this is a simplified test.\n",
        "\n",
        "            # Get the indices of the top K predicted items\n",
        "            _, top_k_indices = scores.topk(k=k)\n",
        "\n",
        "            # Check if the target item is in the top K\n",
        "            is_hit = (top_k_indices == target_item).any().item()\n",
        "            total_recall += is_hit\n",
        "\n",
        "            # Calculate NDCG@K\n",
        "            # Determine the rank of the target item\n",
        "            rank = (scores.argsort(descending=True) == target_item).nonzero(as_tuple=True)[0].item()\n",
        "            if rank < k:\n",
        "                # DCG is 1 / log2(rank + 1). IDCG is 1 (perfect score).\n",
        "                ndcg = 1.0 / np.log2(rank + 2)\n",
        "                total_ndcg += ndcg\n",
        "\n",
        "            total_users += 1\n",
        "\n",
        "    recall_at_k = total_recall / total_users if total_users > 0 else 0\n",
        "    ndcg_at_k = total_ndcg / total_users if total_users > 0 else 0\n",
        "\n",
        "    return recall_at_k, ndcg_at_k\n",
        "\n",
        "# --- Latency Measurement Function ---\n",
        "def measure_latency(model, num_users, num_items, num_tests):\n",
        "    model.eval()\n",
        "    times = []\n",
        "\n",
        "    # Create test batch: 1 user predicting all items\n",
        "    user_id = 0 # Use the first user for consistency\n",
        "    user_tensor = torch.tensor([user_id] * num_items, dtype=torch.long).to(device)\n",
        "    all_item_tensor = torch.arange(num_items, dtype=torch.long).to(device)\n",
        "\n",
        "    # Warm-up runs\n",
        "    for _ in range(10):\n",
        "        _ = model(user_tensor, all_item_tensor)\n",
        "\n",
        "    # Timed runs\n",
        "    for _ in range(num_tests):\n",
        "        start_time = time.time()\n",
        "        _ = model(user_tensor, all_item_tensor)\n",
        "        end_time = time.time()\n",
        "        times.append(end_time - start_time)\n",
        "\n",
        "    avg_latency_ms = (sum(times) / num_tests) * 1000\n",
        "    return avg_latency_ms"
      ],
      "metadata": {
        "id": "jtikCWQQB14L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time # Ensure time module is imported\n",
        "\n",
        "results = []\n",
        "\n",
        "models = {\n",
        "    \"Teacher (Embed=64)\": trained_teacher,\n",
        "    \"Standard Student (Embed=32)\": trained_standard_student,\n",
        "    \"Distilled Student (Embed=32)\": trained_distilled_student,\n",
        "}\n",
        "\n",
        "print(\"\\n--- Starting Empirical Comparison ---\")\n",
        "\n",
        "for name, model in models.items():\n",
        "    # 1. Performance Metrics\n",
        "    recall, ndcg = evaluate_model(model, interactions, actual_num_users, actual_num_items, K)\n",
        "\n",
        "    # 2. Efficiency Metrics\n",
        "    latency = measure_latency(model, actual_num_users, actual_num_items, NUM_LATENCY_TESTS)\n",
        "    params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Recall@K\": recall,\n",
        "        \"NDCG@K\": ndcg,\n",
        "        \"Latency (ms)\": latency,\n",
        "        \"Params\": f\"{params:,}\"\n",
        "    })\n",
        "\n",
        "# --- Final Comparison Table ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"| FINAL KNOWLEDGE DISTILLATION RESULTS (K={K}) |\")\n",
        "print(\"=\"*80)\n",
        "print(f\"| {'Model':<30} | {'Recall@{K}':<10} | {'NDCG@{K}':<10} | {'Latency (ms)':<15} | {'Parameters':<10} |\".replace('{K}', str(K)))\n",
        "print(\"-\"*80)\n",
        "\n",
        "for r in results:\n",
        "    print(f\"| {r['Model']:<30} | {r['Recall@K']:.4f} | {r['NDCG@K']:.4f} | {r['Latency (ms)']:.3f} | {r['Params']:<10} |\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Critical Insight\n",
        "print(\"\\nðŸ”¥ **CRITICAL INSIGHT for Paper:**\")\n",
        "print(\"The Distilled Student must show Recall@K close to the Teacher, AND Latency close to the Standard Student.\")\n",
        "print(\"This proves the successful transfer of high-fidelity knowledge without the computational overhead.\")"
      ],
      "metadata": {
        "id": "i_C3yKHaB5If"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}